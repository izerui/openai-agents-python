# Pipelines and workflows

[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] is a class that makes it easy to turn your agentic workflows into a voice app. You pass in a workflow to run, and the pipeline takes care of transcribing input audio, detecting when the audio ends, calling your workflow at the right time, and turning the workflow output back into audio.
(ã€è¯­éŸ³ç®¡é“å’Œå·¥ä½œæµã€‘
[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] æ˜¯ä¸€ä¸ªç±»ï¼Œå¯ä»¥è½»æ¾å°†æ‚¨çš„ä»£ç†å·¥ä½œæµè½¬æ¢ä¸ºè¯­éŸ³åº”ç”¨ã€‚æ‚¨ä¼ å…¥è¦è¿è¡Œçš„å·¥ä½œæµï¼Œç®¡é“ä¼šè´Ÿè´£è½¬å½•è¾“å…¥éŸ³é¢‘ã€æ£€æµ‹éŸ³é¢‘ä½•æ—¶ç»“æŸã€åœ¨æ­£ç¡®æ—¶é—´è°ƒç”¨æ‚¨çš„å·¥ä½œæµï¼Œå¹¶å°†å·¥ä½œæµè¾“å‡ºè½¬æ¢å›éŸ³é¢‘ã€‚)

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## Configuring a pipeline
(## é…ç½®ç®¡é“)

When you create a pipeline, you can set a few things:
(åˆ›å»ºç®¡é“æ—¶ï¼Œæ‚¨å¯ä»¥è®¾ç½®ä»¥ä¸‹å†…å®¹ï¼š)

1. The [`workflow`][agents.voice.workflow.VoiceWorkflowBase], which is the code that runs each time new audio is transcribed.
(1. [`workflow`][agents.voice.workflow.VoiceWorkflowBase]ï¼Œè¿™æ˜¯æ¯æ¬¡æ–°éŸ³é¢‘è¢«è½¬å½•æ—¶è¿è¡Œçš„ä»£ç )
2. The [`speech-to-text`][agents.voice.model.STTModel] and [`text-to-speech`][agents.voice.model.TTSModel] models used
(2. ä½¿ç”¨çš„[`speech-to-text`][agents.voice.model.STTModel]å’Œ[`text-to-speech`][agents.voice.model.TTSModel]æ¨¡å‹)
3. The [`config`][agents.voice.pipeline_config.VoicePipelineConfig], which lets you configure things like:
(3. [`config`][agents.voice.pipeline_config.VoicePipelineConfig]ï¼Œå…è®¸æ‚¨é…ç½®ä»¥ä¸‹å†…å®¹ï¼š)
    - A model provider, which can map model names to models
    (   - æ¨¡å‹æä¾›è€…ï¼Œå¯ä»¥å°†æ¨¡å‹åç§°æ˜ å°„åˆ°æ¨¡å‹)
    - Tracing, including whether to disable tracing, whether audio files are uploaded, the workflow name, trace IDs etc.
    (   - è¿½è¸ªï¼ŒåŒ…æ‹¬æ˜¯å¦ç¦ç”¨è¿½è¸ªã€æ˜¯å¦ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ã€å·¥ä½œæµåç§°ã€è¿½è¸ªIDç­‰)
    - Settings on the TTS and STT models, like the prompt, language and data types used.
    (   - TTSå’ŒSTTæ¨¡å‹çš„è®¾ç½®ï¼Œå¦‚æç¤ºè¯ã€è¯­è¨€å’Œä½¿ç”¨çš„æ•°æ®ç±»å‹)

## Running a pipeline
(## è¿è¡Œç®¡é“)

You can run a pipeline via the [`run()`][agents.voice.pipeline.VoicePipeline.run] method, which lets you pass in audio input in two forms:
(æ‚¨å¯ä»¥é€šè¿‡[`run()`][agents.voice.pipeline.VoicePipeline.run]æ–¹æ³•è¿è¡Œç®¡é“ï¼Œè¯¥æ–¹æ³•å…è®¸æ‚¨ä»¥ä¸¤ç§å½¢å¼ä¼ å…¥éŸ³é¢‘è¾“å…¥ï¼š)

1. [`AudioInput`][agents.voice.input.AudioInput] is used when you have a full audio transcript, and just want to produce a result for it. This is useful in cases where you don't need to detect when a speaker is done speaking; for example, when you have pre-recorded audio or in push-to-talk apps where it's clear when the user is done speaking.
(1. [`AudioInput`][agents.voice.input.AudioInput]ç”¨äºå½“æ‚¨æœ‰å®Œæ•´çš„éŸ³é¢‘è½¬å½•æ—¶ï¼Œåªéœ€ä¸ºå…¶ç”Ÿæˆç»“æœã€‚è¿™åœ¨ä¸éœ€è¦æ£€æµ‹è¯´è¯è€…ä½•æ—¶è¯´å®Œçš„æƒ…å†µä¸‹å¾ˆæœ‰ç”¨ï¼›ä¾‹å¦‚ï¼Œå½“æ‚¨æœ‰é¢„å½•åˆ¶çš„éŸ³é¢‘æˆ–åœ¨æŒ‰ä¸‹è¯´è¯çš„åº”ç”¨ç¨‹åºä¸­ï¼Œç”¨æˆ·ä½•æ—¶è¯´å®Œå¾ˆæ¸…æ¥š)
2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] is used when you might need to detect when a user is done speaking. It allows you to push audio chunks as they are detected, and the voice pipeline will automatically run the agent workflow at the right time, via a process called "activity detection".
(2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]ç”¨äºå½“æ‚¨å¯èƒ½éœ€è¦æ£€æµ‹ç”¨æˆ·ä½•æ—¶è¯´å®Œæ—¶ã€‚å®ƒå…è®¸æ‚¨åœ¨æ£€æµ‹åˆ°éŸ³é¢‘å—æ—¶æ¨é€å®ƒä»¬ï¼Œè¯­éŸ³ç®¡é“å°†é€šè¿‡ç§°ä¸º"æ´»åŠ¨æ£€æµ‹"çš„è¿‡ç¨‹åœ¨æ­£ç¡®æ—¶é—´è‡ªåŠ¨è¿è¡Œä»£ç†å·¥ä½œæµ)

## Results
(## ç»“æœ)

The result of a voice pipeline run is a [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]. This is an object that lets you stream events as they occur. There are a few kinds of [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent], including:
(è¯­éŸ³ç®¡é“è¿è¡Œçš„ç»“æœæ˜¯ä¸€ä¸ª[`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]ã€‚è¿™æ˜¯ä¸€ä¸ªå…è®¸æ‚¨åœ¨äº‹ä»¶å‘ç”Ÿæ—¶æµå¼ä¼ è¾“çš„å¯¹è±¡ã€‚æœ‰å‡ ç§[`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent]ï¼ŒåŒ…æ‹¬ï¼š)

1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio], which contains a chunk of audio.
(1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]ï¼ŒåŒ…å«ä¸€æ®µéŸ³é¢‘)
2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle], which informs you of lifecycle events like a turn starting or ending.
(2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]ï¼Œé€šçŸ¥æ‚¨ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ï¼Œå¦‚è½®æ¬¡å¼€å§‹æˆ–ç»“æŸ)
3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError], is an error event.
(3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]ï¼Œæ˜¯ä¸€ä¸ªé”™è¯¯äº‹ä»¶)

```python

result = await pipeline.run(input)

async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        # play audio
    elif event.type == "voice_stream_event_lifecycle":
        # lifecycle
    elif event.type == "voice_stream_event_error"
        # error
    ...
```

## Best practices
(## æœ€ä½³å®è·µ)

### Interruptions
(### ä¸­æ–­)

The Agents SDK currently does not support any built-in interruptions support for [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]. Instead for every detected turn it will trigger a separate run of your workflow. If you want to handle interruptions inside your application you can listen to the [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] events. `turn_started` will indicate that a new turn was transcribed and processing is beginning. `turn_ended` will trigger after all the audio was dispatched for a respective turn. You could use these events to mute the microphone of the speaker when the model starts a turn and unmute it after you flushed all the related audio for a turn.
(Agents SDKç›®å‰ä¸æ”¯æŒå¯¹[`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]çš„ä»»ä½•å†…ç½®ä¸­æ–­æ”¯æŒã€‚ç›¸åï¼Œå¯¹äºæ¯ä¸ªæ£€æµ‹åˆ°çš„è½®æ¬¡ï¼Œå®ƒå°†è§¦å‘å·¥ä½œæµçš„å•ç‹¬è¿è¡Œã€‚å¦‚æœæ‚¨æƒ³åœ¨åº”ç”¨ç¨‹åºå†…å¤„ç†ä¸­æ–­ï¼Œå¯ä»¥ç›‘å¬[`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]äº‹ä»¶ã€‚`turn_started`å°†æŒ‡ç¤ºæ–°è½®æ¬¡å·²è¢«è½¬å½•ä¸”å¤„ç†å¼€å§‹ã€‚`turn_ended`å°†åœ¨æ‰€æœ‰éŸ³é¢‘è¢«åˆ†æ´¾ç»™ç›¸åº”è½®æ¬¡åè§¦å‘ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›äº‹ä»¶åœ¨æ¨¡å‹å¼€å§‹è½®æ¬¡æ—¶é™éŸ³æ‰¬å£°å™¨çš„éº¦å…‹é£ï¼Œå¹¶åœ¨åˆ·æ–°è½®æ¬¡çš„æ‰€æœ‰ç›¸å…³éŸ³é¢‘åå–æ¶ˆé™éŸ³)
