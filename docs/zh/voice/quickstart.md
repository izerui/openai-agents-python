# Quickstart (å¿«é€Ÿå…¥é—¨)

## Prerequisites (å…ˆå†³æ¡ä»¶)

Make sure you've followed the base [quickstart instructions](../quickstart.md) for the Agents SDK, and set up a virtual environment. Then, install the optional voice dependencies from the SDK: (è¯·ç¡®ä¿å·²æŒ‰ç…§Agents SDKçš„åŸºç¡€[å¿«é€Ÿå…¥é—¨æŒ‡å—](../quickstart.md)æ“ä½œï¼Œå¹¶è®¾ç½®å¥½è™šæ‹Ÿç¯å¢ƒã€‚ç„¶åä»SDKå®‰è£…å¯é€‰çš„è¯­éŸ³ä¾èµ–é¡¹ï¼š)

```bash
pip install 'openai-agents[voice]'
```

## Concepts (æ¦‚å¿µ)

The main concept to know about is a [`VoicePipeline`][agents.voice.pipeline.VoicePipeline], which is a 3 step process: (ä¸»è¦éœ€è¦äº†è§£çš„æ¦‚å¿µæ˜¯[`VoicePipeline`][agents.voice.pipeline.VoicePipeline]ï¼Œå®ƒæ˜¯ä¸€ä¸ªä¸‰æ­¥æµç¨‹ï¼š)

1. Run a speech-to-text model to turn audio into text. (è¿è¡Œè¯­éŸ³è½¬æ–‡æœ¬æ¨¡å‹å°†éŸ³é¢‘è½¬ä¸ºæ–‡æœ¬)
2. Run your code, which is usually an agentic workflow, to produce a result. (è¿è¡Œä½ çš„ä»£ç ï¼Œé€šå¸¸æ˜¯æ™ºèƒ½ä½“å·¥ä½œæµï¼Œç”Ÿæˆç»“æœ)
3. Run a text-to-speech model to turn the result text back into audio. (è¿è¡Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹å°†ç»“æœæ–‡æœ¬è½¬å›éŸ³é¢‘)

```mermaid
graph LR
    %% Input
    A["ğŸ¤ Audio Input"]

    %% Voice Pipeline
    subgraph Voice_Pipeline [Voice Pipeline]
        direction TB
        B["Transcribe (speech-to-text)"]
        C["Your Code"]:::highlight
        D["Text-to-speech"]
        B --> C --> D
    end

    %% Output
    E["ğŸ§ Audio Output"]

    %% Flow
    A --> Voice_Pipeline
    Voice_Pipeline --> E

    %% Custom styling
    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;

```

## Agents (æ™ºèƒ½ä½“)

First, let's set up some Agents. This should feel familiar to you if you've built any agents with this SDK. We'll have a couple of Agents, a handoff, and a tool. (é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€äº›æ™ºèƒ½ä½“ã€‚å¦‚æœä½ ä½¿ç”¨è¿‡è¿™ä¸ªSDKæ„å»ºè¿‡æ™ºèƒ½ä½“ï¼Œåº”è¯¥ä¼šæ„Ÿåˆ°ç†Ÿæ‚‰ã€‚æˆ‘ä»¬å°†åˆ›å»ºå‡ ä¸ªæ™ºèƒ½ä½“ã€ä¸€ä¸ªäº¤æ¥å’Œä¸€ä¸ªå·¥å…·ã€‚)

```python
import asyncio
import random

from agents import (
    Agent,
    function_tool,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4o-mini",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4o-mini",
    handoffs=[spanish_agent],
    tools=[get_weather],
)
```

## Voice pipeline (è¯­éŸ³ç®¡é“)

We'll set up a simple voice pipeline, using [`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow] as the workflow. (æˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ªç®€å•çš„è¯­éŸ³ç®¡é“ï¼Œä½¿ç”¨[`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow]ä½œä¸ºå·¥ä½œæµã€‚)

```python
from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline
pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
```

## Run the pipeline (è¿è¡Œç®¡é“)

```python
import numpy as np
import sounddevice as sd
from agents.voice import AudioInput

# For simplicity, we'll just create 3 seconds of silence
# In reality, you'd get microphone data
buffer = np.zeros(24000 * 3, dtype=np.int16)
audio_input = AudioInput(buffer=buffer)

result = await pipeline.run(audio_input)

# Create an audio player using `sounddevice`
player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
player.start()

# Play the audio stream as it comes in
async for event in result.stream():
    if event.type == "voice_stream_event_audio":
        player.write(event.data)

```

## Put it all together (å®Œæ•´ç¤ºä¾‹)

```python
import asyncio
import random

import numpy as np
import sounddevice as sd

from agents import (
    Agent,
    function_tool,
    set_tracing_disabled,
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline,
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    print(f"[debug] get_weather called with city: {city}")
    choices = ["sunny", "cloudy", "rainy", "snowy"]
    return f"The weather in {city} is {random.choice(choices)}."


spanish_agent = Agent(
    name="Spanish",
    handoff_description="A spanish speaking agent.",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. Speak in Spanish.",
    ),
    model="gpt-4o-mini",
)

agent = Agent(
    name="Assistant",
    instructions=prompt_with_handoff_instructions(
        "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
    ),
    model="gpt-4o-mini",
    handoffs=[spanish_agent],
    tools=[get_weather],
)


async def main():
    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))
    buffer = np.zeros(24000 * 3, dtype=np.int16)
    audio_input = AudioInput(buffer=buffer)

    result = await pipeline.run(audio_input)

    # Create an audio player using `sounddevice`
    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
    player.start()

    # Play the audio stream as it comes in
    async for event in result.stream():
        if event.type == "voice_stream_event_audio":
            player.write(event.data)


if __name__ == "__main__":
    asyncio.run(main())
```

If you run this example, the agent will speak to you! Check out the example in [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) to see a demo where you can speak to the agent yourself. (å¦‚æœè¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œæ™ºèƒ½ä½“ä¼šä¸ä½ å¯¹è¯ï¼æŸ¥çœ‹[examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static)ä¸­çš„ç¤ºä¾‹ï¼Œå¯ä»¥çœ‹åˆ°ä¸€ä¸ªå¯ä»¥ä¸æ™ºèƒ½ä½“å¯¹è¯çš„æ¼”ç¤ºã€‚)
